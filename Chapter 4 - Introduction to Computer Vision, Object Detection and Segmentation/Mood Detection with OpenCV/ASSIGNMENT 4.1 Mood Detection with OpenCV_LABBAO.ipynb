{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOnC0Lrm3-tF"
      },
      "source": [
        "# Assignment 4.1 - Mood Detection with OpenCV\n",
        "\n",
        "Submitted By: Labbao, Benedick D.<br>\n",
        "Performed On: 03/21/2024<br>\n",
        "Submitted On: 03/22/2024\n",
        "\n",
        "Submitted To: Engr. Roman M. Richard\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOCQf_Jh4Fnj"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g_OM2fwYjS8o"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "raw_dataset_path = 'dataset/faces/'\n",
        "preprocessed_dataset_path = 'dataset/preprocessed_faces/'\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade/haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('haarcascade/haarcascade_eye.xml')\n",
        "\n",
        "def display_image(image, title=None, conversion=cv2.COLOR_BGR2RGB):\n",
        "    image = cv2.cvtColor(image, conversion)\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def fix_dataset_names(directory, prefix='face_'):\n",
        "    files = os.listdir(directory)\n",
        "    counter = 0\n",
        "    \n",
        "    for file in files:\n",
        "        _, ext = os.path.splitext(file)\n",
        "        new_name = f\"{prefix}{counter:04d}{ext}\"\n",
        "        print('fixing', new_name)\n",
        "        \n",
        "        os.rename(os.path.join(directory, file), os.path.join(directory, new_name))\n",
        "        counter += 1\n",
        "\n",
        "def generate_name(directory):\n",
        "    files = os.listdir(directory)\n",
        "    num_files = len(files)\n",
        "    face_names = 'face_' + str(num_files).zfill(4)\n",
        "    return face_names + '.png'\n",
        "\n",
        "def capture(count=1):\n",
        "    camera = cv2.VideoCapture(0)\n",
        "\n",
        "    while (count > 0):\n",
        "        ret, frame = camera.read()\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
        "\n",
        "        for (x,y,w,h) in faces:\n",
        "            image = frame[y:(y+h), x:(x+w)]\n",
        "            name = generate_name(raw_dataset_path)\n",
        "            cv2.imwrite(raw_dataset_path + name, image)\n",
        "            print(f'saving {name}...')\n",
        "\n",
        "        sleep(0.3)\n",
        "        count-=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fixing sad_0000.png\n",
            "fixing sad_0001.png\n",
            "fixing sad_0002.png\n",
            "fixing sad_0003.png\n",
            "fixing sad_0004.png\n",
            "fixing sad_0005.png\n",
            "fixing sad_0006.png\n",
            "fixing sad_0007.png\n",
            "fixing sad_0008.png\n",
            "fixing sad_0009.png\n",
            "fixing sad_0010.png\n",
            "fixing sad_0011.png\n",
            "fixing sad_0012.png\n",
            "fixing sad_0013.png\n",
            "fixing sad_0014.png\n",
            "fixing sad_0015.png\n",
            "fixing sad_0016.png\n",
            "fixing sad_0017.png\n",
            "fixing sad_0018.png\n",
            "fixing sad_0019.png\n",
            "fixing sad_0020.png\n",
            "fixing sad_0021.png\n",
            "fixing sad_0022.png\n",
            "fixing sad_0023.png\n",
            "fixing sad_0024.png\n",
            "fixing sad_0025.png\n",
            "fixing sad_0026.png\n",
            "fixing sad_0027.png\n",
            "fixing sad_0028.png\n",
            "fixing sad_0029.png\n",
            "fixing sad_0030.png\n",
            "fixing sad_0031.png\n",
            "fixing sad_0032.png\n",
            "fixing sad_0033.png\n",
            "fixing sad_0034.png\n",
            "fixing sad_0035.png\n",
            "fixing sad_0036.png\n",
            "fixing sad_0037.png\n",
            "fixing sad_0038.png\n",
            "fixing sad_0039.png\n",
            "fixing sad_0040.png\n",
            "fixing sad_0041.png\n",
            "fixing sad_0042.png\n",
            "fixing sad_0043.png\n",
            "fixing sad_0044.png\n",
            "fixing sad_0045.png\n",
            "fixing sad_0046.png\n",
            "fixing sad_0047.png\n",
            "fixing sad_0048.png\n",
            "fixing sad_0049.png\n",
            "fixing sad_0050.png\n",
            "fixing sad_0051.png\n",
            "fixing sad_0052.png\n",
            "fixing sad_0053.png\n",
            "fixing sad_0054.png\n",
            "fixing sad_0055.png\n",
            "fixing sad_0056.png\n",
            "fixing sad_0057.png\n",
            "fixing sad_0058.png\n",
            "fixing sad_0059.png\n",
            "fixing sad_0060.png\n",
            "fixing sad_0061.png\n",
            "fixing sad_0062.png\n",
            "fixing sad_0063.png\n",
            "fixing sad_0064.png\n",
            "fixing sad_0065.png\n",
            "fixing sad_0066.png\n",
            "fixing sad_0067.png\n",
            "fixing sad_0068.png\n",
            "fixing sad_0069.png\n",
            "fixing sad_0070.png\n",
            "fixing sad_0071.png\n",
            "fixing sad_0072.png\n",
            "fixing sad_0073.png\n",
            "fixing sad_0074.png\n",
            "fixing sad_0075.png\n",
            "fixing sad_0076.png\n",
            "fixing sad_0077.png\n",
            "fixing sad_0078.png\n",
            "fixing sad_0079.png\n",
            "fixing sad_0080.png\n",
            "fixing sad_0081.png\n",
            "fixing sad_0082.png\n",
            "fixing sad_0083.png\n",
            "fixing sad_0084.png\n",
            "fixing sad_0085.png\n",
            "fixing sad_0086.png\n",
            "fixing sad_0087.png\n",
            "fixing sad_0088.png\n",
            "fixing sad_0089.png\n",
            "fixing sad_0090.png\n",
            "fixing sad_0091.png\n",
            "fixing sad_0092.png\n",
            "fixing sad_0093.png\n",
            "fixing sad_0094.png\n",
            "fixing sad_0095.png\n",
            "fixing sad_0096.png\n",
            "fixing sad_0097.png\n",
            "fixing sad_0098.png\n",
            "fixing sad_0099.png\n"
          ]
        }
      ],
      "source": [
        "fix_dataset_names(raw_dataset_path + 'sad/', 'sad_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saving face_0004.png...\n",
            "saving face_0005.png...\n",
            "saving face_0006.png...\n",
            "saving face_0007.png...\n",
            "saving face_0008.png...\n",
            "saving face_0009.png...\n",
            "saving face_0010.png...\n",
            "saving face_0011.png...\n",
            "saving face_0012.png...\n",
            "saving face_0013.png...\n",
            "saving face_0014.png...\n",
            "saving face_0015.png...\n",
            "saving face_0016.png...\n",
            "saving face_0017.png...\n",
            "saving face_0018.png...\n",
            "saving face_0019.png...\n",
            "saving face_0020.png...\n",
            "saving face_0021.png...\n",
            "saving face_0022.png...\n",
            "saving face_0023.png...\n",
            "saving face_0024.png...\n",
            "saving face_0025.png...\n",
            "saving face_0026.png...\n",
            "saving face_0027.png...\n",
            "saving face_0028.png...\n",
            "saving face_0029.png...\n",
            "saving face_0030.png...\n",
            "saving face_0031.png...\n",
            "saving face_0032.png...\n",
            "saving face_0033.png...\n",
            "saving face_0034.png...\n",
            "saving face_0035.png...\n",
            "saving face_0036.png...\n",
            "saving face_0037.png...\n",
            "saving face_0038.png...\n",
            "saving face_0039.png...\n",
            "saving face_0040.png...\n",
            "saving face_0041.png...\n",
            "saving face_0042.png...\n",
            "saving face_0043.png...\n",
            "saving face_0044.png...\n",
            "saving face_0045.png...\n",
            "saving face_0046.png...\n",
            "saving face_0047.png...\n",
            "saving face_0048.png...\n",
            "saving face_0049.png...\n",
            "saving face_0050.png...\n",
            "saving face_0051.png...\n",
            "saving face_0052.png...\n",
            "saving face_0053.png...\n",
            "saving face_0054.png...\n",
            "saving face_0055.png...\n",
            "saving face_0056.png...\n",
            "saving face_0057.png...\n",
            "saving face_0058.png...\n",
            "saving face_0059.png...\n",
            "saving face_0060.png...\n",
            "saving face_0061.png...\n",
            "saving face_0062.png...\n",
            "saving face_0063.png...\n",
            "saving face_0064.png...\n",
            "saving face_0065.png...\n",
            "saving face_0066.png...\n",
            "saving face_0067.png...\n",
            "saving face_0068.png...\n",
            "saving face_0069.png...\n",
            "saving face_0070.png...\n",
            "saving face_0071.png...\n",
            "saving face_0072.png...\n",
            "saving face_0073.png...\n",
            "saving face_0074.png...\n",
            "saving face_0075.png...\n",
            "saving face_0076.png...\n",
            "saving face_0077.png...\n",
            "saving face_0078.png...\n",
            "saving face_0079.png...\n",
            "saving face_0080.png...\n",
            "saving face_0081.png...\n",
            "saving face_0082.png...\n",
            "saving face_0083.png...\n",
            "saving face_0084.png...\n",
            "saving face_0085.png...\n",
            "saving face_0086.png...\n",
            "saving face_0087.png...\n",
            "saving face_0088.png...\n",
            "saving face_0089.png...\n",
            "saving face_0090.png...\n",
            "saving face_0091.png...\n",
            "saving face_0092.png...\n",
            "saving face_0093.png...\n",
            "saving face_0094.png...\n",
            "saving face_0095.png...\n",
            "saving face_0096.png...\n",
            "saving face_0097.png...\n",
            "saving face_0098.png...\n",
            "saving face_0099.png...\n",
            "saving face_0100.png...\n",
            "saving face_0101.png...\n",
            "saving face_0102.png...\n",
            "saving face_0103.png...\n"
          ]
        }
      ],
      "source": [
        "capture(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will take face images at an random interval to create a dataset for facial recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove background by locating eyes and extracting face region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_face_region(image):\n",
        "    center_point = lambda p1, p2: (int((p1[0] + p2[0]) / 2)+3, int((p1[1] + p2[1]) / 2))\n",
        "\n",
        "    if len(image.shape) == 2:\n",
        "        gray = image\n",
        "    else:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    eyes = eye_cascade.detectMultiScale(gray, 1.03, 15, 0, (40, 40))\n",
        "\n",
        "    # Perform eye detection then find center point of left and right eyes\n",
        "    eyes_center_points = []\n",
        "    for (x,y,w,h) in eyes:\n",
        "        eyes_center_points.append(center_point((x,y), (x+w, y+h)))\n",
        "        # cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 1)\n",
        "        # display_image(gray)\n",
        "        \n",
        "    # Find center betweem right eye's center point and left eye's center point\n",
        "    if len(eyes_center_points) < 2:\n",
        "        return image\n",
        "\n",
        "    eye_center = center_point(eyes_center_points[0], eyes_center_points[1])\n",
        "    x, y = eye_center\n",
        "\n",
        "    # add offset to create a bounding box that only contains face region\n",
        "    x1, y1, x2, y2 = x - 100, y + 140, x + 100, y - 60\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2),  (0, 255, 0), 2)\n",
        "\n",
        "    return image[y2:y1, x1:x2]\n",
        "\n",
        "def resize(image):\n",
        "    w, h = image.shape\n",
        "    new_image = cv2.resize(image, (140, 140))\n",
        "    return new_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_images(src_dir, dst_dir):\n",
        "    files = os.listdir(src_dir)\n",
        "    \n",
        "    for file in files:\n",
        "        file_name, ext = os.path.splitext(file)\n",
        "        print(\"Processing image:\", file_name + ext)\n",
        "\n",
        "        image = cv2.imread(src_dir + file_name + ext)\n",
        "        image = extract_face_region(image)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        cv2.imwrite(dst_dir + file_name + ext, resize(gray))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we created a function to perform preprocessing on all images and move them in a separate folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing image: happy_0000.png\n",
            "Processing image: happy_0001.png\n",
            "Processing image: happy_0002.png\n",
            "Processing image: happy_0003.png\n",
            "Processing image: happy_0004.png\n",
            "Processing image: happy_0005.png\n",
            "Processing image: happy_0006.png\n",
            "Processing image: happy_0007.png\n",
            "Processing image: happy_0008.png\n",
            "Processing image: happy_0009.png\n",
            "Processing image: happy_0010.png\n",
            "Processing image: happy_0011.png\n",
            "Processing image: happy_0012.png\n",
            "Processing image: happy_0013.png\n",
            "Processing image: happy_0014.png\n",
            "Processing image: happy_0015.png\n",
            "Processing image: happy_0016.png\n",
            "Processing image: happy_0017.png\n",
            "Processing image: happy_0018.png\n",
            "Processing image: happy_0019.png\n",
            "Processing image: happy_0020.png\n",
            "Processing image: happy_0021.png\n",
            "Processing image: happy_0022.png\n",
            "Processing image: happy_0023.png\n",
            "Processing image: happy_0024.png\n",
            "Processing image: happy_0025.png\n",
            "Processing image: happy_0026.png\n",
            "Processing image: happy_0027.png\n",
            "Processing image: happy_0028.png\n",
            "Processing image: happy_0029.png\n",
            "Processing image: happy_0030.png\n",
            "Processing image: happy_0031.png\n",
            "Processing image: happy_0032.png\n",
            "Processing image: happy_0033.png\n",
            "Processing image: happy_0034.png\n",
            "Processing image: happy_0035.png\n",
            "Processing image: happy_0036.png\n",
            "Processing image: happy_0037.png\n",
            "Processing image: happy_0038.png\n",
            "Processing image: happy_0039.png\n",
            "Processing image: happy_0040.png\n",
            "Processing image: happy_0041.png\n",
            "Processing image: happy_0042.png\n",
            "Processing image: happy_0043.png\n",
            "Processing image: happy_0044.png\n",
            "Processing image: happy_0045.png\n",
            "Processing image: happy_0046.png\n",
            "Processing image: happy_0047.png\n",
            "Processing image: happy_0048.png\n",
            "Processing image: happy_0049.png\n",
            "Processing image: happy_0050.png\n",
            "Processing image: happy_0051.png\n",
            "Processing image: happy_0052.png\n",
            "Processing image: happy_0053.png\n",
            "Processing image: happy_0054.png\n",
            "Processing image: happy_0055.png\n",
            "Processing image: happy_0056.png\n",
            "Processing image: happy_0057.png\n",
            "Processing image: happy_0058.png\n",
            "Processing image: happy_0059.png\n",
            "Processing image: happy_0060.png\n",
            "Processing image: happy_0061.png\n",
            "Processing image: happy_0062.png\n",
            "Processing image: happy_0063.png\n",
            "Processing image: happy_0064.png\n",
            "Processing image: happy_0065.png\n",
            "Processing image: happy_0066.png\n",
            "Processing image: happy_0067.png\n",
            "Processing image: happy_0068.png\n",
            "Processing image: happy_0069.png\n",
            "Processing image: happy_0070.png\n",
            "Processing image: happy_0071.png\n",
            "Processing image: happy_0072.png\n",
            "Processing image: happy_0073.png\n",
            "Processing image: happy_0074.png\n",
            "Processing image: happy_0075.png\n",
            "Processing image: happy_0076.png\n",
            "Processing image: happy_0077.png\n",
            "Processing image: happy_0078.png\n",
            "Processing image: happy_0079.png\n",
            "Processing image: happy_0080.png\n",
            "Processing image: happy_0081.png\n",
            "Processing image: happy_0082.png\n",
            "Processing image: happy_0083.png\n",
            "Processing image: happy_0084.png\n",
            "Processing image: happy_0085.png\n",
            "Processing image: happy_0086.png\n",
            "Processing image: happy_0087.png\n",
            "Processing image: happy_0088.png\n",
            "Processing image: happy_0089.png\n",
            "Processing image: happy_0090.png\n",
            "Processing image: happy_0091.png\n",
            "Processing image: happy_0092.png\n",
            "Processing image: happy_0093.png\n",
            "Processing image: happy_0094.png\n",
            "Processing image: happy_0095.png\n",
            "Processing image: happy_0096.png\n",
            "Processing image: happy_0097.png\n",
            "Processing image: happy_0098.png\n",
            "Processing image: happy_0099.png\n"
          ]
        }
      ],
      "source": [
        "preprocess_images(raw_dataset_path + 'angry/', preprocessed_dataset_path + 'angry/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing image: happy_0000.png\n",
            "Processing image: happy_0001.png\n",
            "Processing image: happy_0002.png\n",
            "Processing image: happy_0003.png\n",
            "Processing image: happy_0004.png\n",
            "Processing image: happy_0005.png\n",
            "Processing image: happy_0006.png\n",
            "Processing image: happy_0007.png\n",
            "Processing image: happy_0008.png\n",
            "Processing image: happy_0009.png\n",
            "Processing image: happy_0010.png\n",
            "Processing image: happy_0011.png\n",
            "Processing image: happy_0012.png\n",
            "Processing image: happy_0013.png\n",
            "Processing image: happy_0014.png\n",
            "Processing image: happy_0015.png\n",
            "Processing image: happy_0016.png\n",
            "Processing image: happy_0017.png\n",
            "Processing image: happy_0018.png\n",
            "Processing image: happy_0019.png\n",
            "Processing image: happy_0020.png\n",
            "Processing image: happy_0021.png\n",
            "Processing image: happy_0022.png\n",
            "Processing image: happy_0023.png\n",
            "Processing image: happy_0024.png\n",
            "Processing image: happy_0025.png\n",
            "Processing image: happy_0026.png\n",
            "Processing image: happy_0027.png\n",
            "Processing image: happy_0028.png\n",
            "Processing image: happy_0029.png\n",
            "Processing image: happy_0030.png\n",
            "Processing image: happy_0031.png\n",
            "Processing image: happy_0032.png\n",
            "Processing image: happy_0033.png\n",
            "Processing image: happy_0034.png\n",
            "Processing image: happy_0035.png\n",
            "Processing image: happy_0036.png\n",
            "Processing image: happy_0037.png\n",
            "Processing image: happy_0038.png\n",
            "Processing image: happy_0039.png\n",
            "Processing image: happy_0040.png\n",
            "Processing image: happy_0041.png\n",
            "Processing image: happy_0042.png\n",
            "Processing image: happy_0043.png\n",
            "Processing image: happy_0044.png\n",
            "Processing image: happy_0045.png\n",
            "Processing image: happy_0046.png\n",
            "Processing image: happy_0047.png\n",
            "Processing image: happy_0048.png\n",
            "Processing image: happy_0049.png\n",
            "Processing image: happy_0050.png\n",
            "Processing image: happy_0051.png\n",
            "Processing image: happy_0052.png\n",
            "Processing image: happy_0053.png\n",
            "Processing image: happy_0054.png\n",
            "Processing image: happy_0055.png\n",
            "Processing image: happy_0056.png\n",
            "Processing image: happy_0057.png\n",
            "Processing image: happy_0058.png\n",
            "Processing image: happy_0059.png\n",
            "Processing image: happy_0060.png\n",
            "Processing image: happy_0061.png\n",
            "Processing image: happy_0062.png\n",
            "Processing image: happy_0063.png\n",
            "Processing image: happy_0064.png\n",
            "Processing image: happy_0065.png\n",
            "Processing image: happy_0066.png\n",
            "Processing image: happy_0067.png\n",
            "Processing image: happy_0068.png\n",
            "Processing image: happy_0069.png\n",
            "Processing image: happy_0070.png\n",
            "Processing image: happy_0071.png\n",
            "Processing image: happy_0072.png\n",
            "Processing image: happy_0073.png\n",
            "Processing image: happy_0074.png\n",
            "Processing image: happy_0075.png\n",
            "Processing image: happy_0076.png\n",
            "Processing image: happy_0077.png\n",
            "Processing image: happy_0078.png\n",
            "Processing image: happy_0079.png\n",
            "Processing image: happy_0080.png\n",
            "Processing image: happy_0081.png\n",
            "Processing image: happy_0082.png\n",
            "Processing image: happy_0083.png\n",
            "Processing image: happy_0084.png\n",
            "Processing image: happy_0085.png\n",
            "Processing image: happy_0086.png\n",
            "Processing image: happy_0087.png\n",
            "Processing image: happy_0088.png\n",
            "Processing image: happy_0089.png\n",
            "Processing image: happy_0090.png\n",
            "Processing image: happy_0091.png\n",
            "Processing image: happy_0092.png\n",
            "Processing image: happy_0093.png\n",
            "Processing image: happy_0094.png\n",
            "Processing image: happy_0095.png\n",
            "Processing image: happy_0096.png\n",
            "Processing image: happy_0097.png\n",
            "Processing image: happy_0098.png\n",
            "Processing image: happy_0099.png\n"
          ]
        }
      ],
      "source": [
        "preprocess_images(raw_dataset_path + 'happy/', preprocessed_dataset_path + 'happy/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess_images(raw_dataset_path + 'sad/', preprocessed_dataset_path + 'sad/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing image: neutral_0000.png\n",
            "Processing image: neutral_0001.png\n",
            "Processing image: neutral_0002.png\n",
            "Processing image: neutral_0003.png\n",
            "Processing image: neutral_0004.png\n",
            "Processing image: neutral_0005.png\n",
            "Processing image: neutral_0006.png\n",
            "Processing image: neutral_0007.png\n",
            "Processing image: neutral_0008.png\n",
            "Processing image: neutral_0009.png\n",
            "Processing image: neutral_0010.png\n",
            "Processing image: neutral_0011.png\n",
            "Processing image: neutral_0012.png\n",
            "Processing image: neutral_0013.png\n",
            "Processing image: neutral_0014.png\n",
            "Processing image: neutral_0015.png\n",
            "Processing image: neutral_0016.png\n",
            "Processing image: neutral_0017.png\n",
            "Processing image: neutral_0018.png\n",
            "Processing image: neutral_0019.png\n",
            "Processing image: neutral_0020.png\n",
            "Processing image: neutral_0021.png\n",
            "Processing image: neutral_0022.png\n",
            "Processing image: neutral_0023.png\n",
            "Processing image: neutral_0024.png\n",
            "Processing image: neutral_0025.png\n",
            "Processing image: neutral_0026.png\n",
            "Processing image: neutral_0027.png\n",
            "Processing image: neutral_0028.png\n",
            "Processing image: neutral_0029.png\n",
            "Processing image: neutral_0030.png\n",
            "Processing image: neutral_0031.png\n",
            "Processing image: neutral_0032.png\n",
            "Processing image: neutral_0033.png\n",
            "Processing image: neutral_0034.png\n",
            "Processing image: neutral_0035.png\n",
            "Processing image: neutral_0036.png\n",
            "Processing image: neutral_0037.png\n",
            "Processing image: neutral_0038.png\n",
            "Processing image: neutral_0039.png\n",
            "Processing image: neutral_0040.png\n",
            "Processing image: neutral_0041.png\n",
            "Processing image: neutral_0042.png\n",
            "Processing image: neutral_0043.png\n",
            "Processing image: neutral_0044.png\n",
            "Processing image: neutral_0045.png\n",
            "Processing image: neutral_0046.png\n",
            "Processing image: neutral_0047.png\n",
            "Processing image: neutral_0048.png\n",
            "Processing image: neutral_0049.png\n",
            "Processing image: neutral_0050.png\n",
            "Processing image: neutral_0051.png\n",
            "Processing image: neutral_0052.png\n",
            "Processing image: neutral_0053.png\n",
            "Processing image: neutral_0054.png\n",
            "Processing image: neutral_0055.png\n",
            "Processing image: neutral_0056.png\n",
            "Processing image: neutral_0057.png\n",
            "Processing image: neutral_0058.png\n",
            "Processing image: neutral_0059.png\n",
            "Processing image: neutral_0060.png\n",
            "Processing image: neutral_0061.png\n",
            "Processing image: neutral_0062.png\n",
            "Processing image: neutral_0063.png\n",
            "Processing image: neutral_0064.png\n",
            "Processing image: neutral_0065.png\n",
            "Processing image: neutral_0066.png\n",
            "Processing image: neutral_0067.png\n",
            "Processing image: neutral_0068.png\n",
            "Processing image: neutral_0069.png\n",
            "Processing image: neutral_0070.png\n",
            "Processing image: neutral_0071.png\n",
            "Processing image: neutral_0072.png\n",
            "Processing image: neutral_0073.png\n",
            "Processing image: neutral_0074.png\n",
            "Processing image: neutral_0075.png\n",
            "Processing image: neutral_0076.png\n",
            "Processing image: neutral_0077.png\n",
            "Processing image: neutral_0078.png\n",
            "Processing image: neutral_0079.png\n",
            "Processing image: neutral_0080.png\n",
            "Processing image: neutral_0081.png\n",
            "Processing image: neutral_0082.png\n",
            "Processing image: neutral_0083.png\n",
            "Processing image: neutral_0084.png\n",
            "Processing image: neutral_0085.png\n",
            "Processing image: neutral_0086.png\n",
            "Processing image: neutral_0087.png\n",
            "Processing image: neutral_0088.png\n",
            "Processing image: neutral_0089.png\n",
            "Processing image: neutral_0090.png\n",
            "Processing image: neutral_0091.png\n",
            "Processing image: neutral_0092.png\n",
            "Processing image: neutral_0093.png\n",
            "Processing image: neutral_0094.png\n",
            "Processing image: neutral_0095.png\n",
            "Processing image: neutral_0096.png\n",
            "Processing image: neutral_0097.png\n",
            "Processing image: neutral_0098.png\n",
            "Processing image: neutral_0099.png\n"
          ]
        }
      ],
      "source": [
        "preprocess_images(raw_dataset_path + 'neutral/', preprocessed_dataset_path + 'neutral/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Face Recognition Model\n",
        "\n",
        "I will be using LBPH Algorithm to create a Face Recognition Model to recognize me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_face_recognition_model(data_dir):\n",
        "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "\n",
        "    # Prepare training data\n",
        "    faces = []\n",
        "    labels = []\n",
        "\n",
        "    # Recursively walk through the directory and its subdirectories\n",
        "    for root, _, files in os.walk(data_dir):\n",
        "        for filename in files:\n",
        "            # Check if the file has an image extension\n",
        "            if any(filename.lower().endswith(ext) for ext in ['.png']):\n",
        "                img = cv2.imread(os.path.join(root, filename), cv2.IMREAD_GRAYSCALE)\n",
        "                faces.append(img)\n",
        "                labels.append('Ben')\n",
        "\n",
        "    # Assign unique integer labels to each person\n",
        "    label_dict = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
        "    labels = [label_dict[label] for label in labels]\n",
        "\n",
        "    # Train the recognizer\n",
        "    recognizer.train(faces, np.array(labels))\n",
        "\n",
        "    # Save the trained model\n",
        "    recognizer.save(\"face_recognition.xml\")\n",
        "    print(\"Face recognition model trained and saved successfully.\")\n",
        "\n",
        "def test_face_recognition_model(image):\n",
        "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "    recognizer.read(\"face_recognition.xml\")\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Use LBPH recognizer to predict the identity of the face in the test image\n",
        "    label, confidence = recognizer.predict(gray_image)\n",
        "\n",
        "    return (label, confidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Face recognition model trained and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "train_face_recognition_model(raw_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, 67.41625512149969)\n"
          ]
        }
      ],
      "source": [
        "image = cv2.imread(preprocessed_dataset_path + '/angry/angry_0000.png')\n",
        "results = test_face_recognition_model(image)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing if the model can recognize processed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 0, Confidence: 60.27\n",
            "ID: 0, Confidence: 60.10\n",
            "ID: 0, Confidence: 58.94\n",
            "ID: 0, Confidence: 63.02\n",
            "ID: 0, Confidence: 60.76\n",
            "ID: 0, Confidence: 55.66\n",
            "ID: 0, Confidence: 49.10\n",
            "ID: 0, Confidence: 51.25\n",
            "ID: 0, Confidence: 50.96\n",
            "ID: 0, Confidence: 50.74\n",
            "ID: 0, Confidence: 51.95\n",
            "ID: 0, Confidence: 78.07\n",
            "ID: 0, Confidence: 72.44\n",
            "ID: 0, Confidence: 73.72\n",
            "ID: 0, Confidence: 69.27\n",
            "ID: 0, Confidence: 62.90\n",
            "ID: 0, Confidence: 62.30\n",
            "ID: 0, Confidence: 60.81\n",
            "ID: 0, Confidence: 61.18\n",
            "ID: 0, Confidence: 61.09\n",
            "ID: 0, Confidence: 60.81\n",
            "ID: 0, Confidence: 58.50\n",
            "ID: 0, Confidence: 70.94\n",
            "ID: 0, Confidence: 70.51\n",
            "ID: 0, Confidence: 67.25\n",
            "ID: 0, Confidence: 76.48\n",
            "ID: 0, Confidence: 64.04\n"
          ]
        }
      ],
      "source": [
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.read(\"face_recognition.xml\")\n",
        "\n",
        "# Initialize the camera\n",
        "\n",
        "cam = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cam.read()\n",
        "\n",
        "    if(frame.all() == None):\n",
        "        continue\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.03, 5)\n",
        "\n",
        "    # Draw rectangles around the detected faces and recognize them\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Recognize the face\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        id_, confidence = recognizer.predict(roi_gray)\n",
        "\n",
        "        # If recognized face belongs to you (adjust confidence threshold as needed)\n",
        "        if confidence < 70:\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            label_text = f\"You (ID: {id_}, Confidence: {confidence:.2f})\"\n",
        "            cv2.putText(frame, label_text, (x-10, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "        else:\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
        "            label_text = f\"Unknown (ID: {id_}, Confidence: {confidence:.2f})\"\n",
        "            cv2.putText(frame, label_text, (x-10, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "        # Print ID and confidence\n",
        "        print(f\"ID: {id_}, Confidence: {confidence:.2f}\")\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow('Face Recognition', frame)\n",
        "\n",
        "    # Break the loop when 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing if it can recognize me using actually images from camera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Facial Expression Recognition Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Input, Dropout, Activation\n",
        "from keras.models import Sequential, load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_training_data(data_dir, test_size=0.2):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Recursively walk through the directory and its subdirectories\n",
        "    for root, _, files in os.walk(data_dir):\n",
        "        for filename in files:\n",
        "            # Check if the file has an image extension\n",
        "            if any(filename.lower().endswith(ext) for ext in ['.png']):\n",
        "                img = cv2.imread(os.path.join(root, filename), cv2.IMREAD_GRAYSCALE)\n",
        "                X.append(img)\n",
        "                l = filename.split('_')\n",
        "                y.append(l[0])\n",
        "\n",
        "    # Assign unique integer labels to each person\n",
        "    label_dict = {label: idx for idx, label in enumerate(np.unique(y))}\n",
        "    y = [label_dict[label] for label in y]\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=10010)\n",
        "\n",
        "    X_train = np.array(X_train).reshape(-1, 140, 140, 1)\n",
        "    X_test = np.array(X_test).reshape(-1, 140, 140, 1)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    print(X_train.shape)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "\n",
        "def train_facial_expression_recognition_model(directory):\n",
        "    train, valid = load_training_data(directory)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(140, 140, 1)))\n",
        "    model.add(Conv2D(32, (5,5), 2))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(16, (5,5), 2))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(16))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(4))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile('adam',\n",
        "                'sparse_categorical_crossentropy', # use sparse since the label is not one-hot encoded\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "\n",
        "    history = model.fit(train[0],\n",
        "                    train[1],\n",
        "                    epochs=50,\n",
        "                    validation_data=valid)\n",
        "    \n",
        "    model.save('facial_expression_recognition.keras')\n",
        "\n",
        "    return history\n",
        "\n",
        "def test_facial_expression_recognition_model(test_data):\n",
        "    model = load_model('facial_expression_recognition.keras')\n",
        "    model.evaluate(test_data[0], test_data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(320, 140, 140, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m832\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │        \u001b[38;5;34m12,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │        \u001b[38;5;34m12,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m68\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,276</span> (102.64 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,276\u001b[0m (102.64 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,276</span> (102.64 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,276\u001b[0m (102.64 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 462ms/step - accuracy: 0.3232 - loss: 19.2610 - val_accuracy: 0.4000 - val_loss: 1.2947\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 255ms/step - accuracy: 0.2690 - loss: 2.4599 - val_accuracy: 0.2750 - val_loss: 1.3864\n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 252ms/step - accuracy: 0.2618 - loss: 1.4633 - val_accuracy: 0.2250 - val_loss: 1.3867\n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 267ms/step - accuracy: 0.2723 - loss: 1.3775 - val_accuracy: 0.2250 - val_loss: 1.3869\n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - accuracy: 0.3145 - loss: 1.3765 - val_accuracy: 0.2250 - val_loss: 1.3870\n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - accuracy: 0.3175 - loss: 1.3704 - val_accuracy: 0.2250 - val_loss: 1.3871\n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - accuracy: 0.3698 - loss: 1.3449 - val_accuracy: 0.2750 - val_loss: 1.3801\n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - accuracy: 0.3907 - loss: 1.3149 - val_accuracy: 0.4250 - val_loss: 1.2626\n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - accuracy: 0.4212 - loss: 1.2259 - val_accuracy: 0.4250 - val_loss: 1.1313\n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 254ms/step - accuracy: 0.4099 - loss: 1.1893 - val_accuracy: 0.6125 - val_loss: 1.1408\n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - accuracy: 0.5071 - loss: 1.0183 - val_accuracy: 0.6500 - val_loss: 0.9683\n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 239ms/step - accuracy: 0.5785 - loss: 0.9080 - val_accuracy: 0.6875 - val_loss: 0.5814\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 244ms/step - accuracy: 0.6239 - loss: 0.7798 - val_accuracy: 0.7625 - val_loss: 0.6736\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - accuracy: 0.6216 - loss: 0.7020 - val_accuracy: 0.6000 - val_loss: 0.5848\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 243ms/step - accuracy: 0.7185 - loss: 0.6430 - val_accuracy: 0.6750 - val_loss: 0.4957\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - accuracy: 0.7154 - loss: 0.6711 - val_accuracy: 0.6875 - val_loss: 0.5104\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - accuracy: 0.7025 - loss: 0.5875 - val_accuracy: 0.7125 - val_loss: 0.5191\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - accuracy: 0.7268 - loss: 0.5539 - val_accuracy: 0.6750 - val_loss: 0.4544\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 261ms/step - accuracy: 0.7545 - loss: 0.4993 - val_accuracy: 0.7625 - val_loss: 0.4194\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - accuracy: 0.7531 - loss: 0.4701 - val_accuracy: 0.8375 - val_loss: 0.4778\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - accuracy: 0.7595 - loss: 0.5618 - val_accuracy: 0.8375 - val_loss: 0.4472\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 246ms/step - accuracy: 0.7457 - loss: 0.4840 - val_accuracy: 0.8500 - val_loss: 0.3269\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 243ms/step - accuracy: 0.7768 - loss: 0.5573 - val_accuracy: 0.9000 - val_loss: 0.3247\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - accuracy: 0.8259 - loss: 0.3838 - val_accuracy: 0.9125 - val_loss: 0.3471\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 242ms/step - accuracy: 0.8078 - loss: 0.3854 - val_accuracy: 0.8875 - val_loss: 0.2633\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - accuracy: 0.8316 - loss: 0.3739 - val_accuracy: 0.9000 - val_loss: 0.2936\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 260ms/step - accuracy: 0.7906 - loss: 0.4396 - val_accuracy: 0.9000 - val_loss: 0.2783\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 251ms/step - accuracy: 0.8425 - loss: 0.4101 - val_accuracy: 0.9625 - val_loss: 0.2044\n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 250ms/step - accuracy: 0.8548 - loss: 0.3246 - val_accuracy: 0.9250 - val_loss: 0.1997\n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 256ms/step - accuracy: 0.8660 - loss: 0.3547 - val_accuracy: 0.9500 - val_loss: 0.1800\n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 245ms/step - accuracy: 0.8980 - loss: 0.2963 - val_accuracy: 0.9750 - val_loss: 0.1113\n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - accuracy: 0.8740 - loss: 0.3386 - val_accuracy: 0.9625 - val_loss: 0.1871\n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - accuracy: 0.8243 - loss: 0.4004 - val_accuracy: 0.9625 - val_loss: 0.1491\n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 244ms/step - accuracy: 0.8663 - loss: 0.3921 - val_accuracy: 0.9750 - val_loss: 0.1121\n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 276ms/step - accuracy: 0.9186 - loss: 0.2434 - val_accuracy: 0.9875 - val_loss: 0.1116\n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 276ms/step - accuracy: 0.9057 - loss: 0.2072 - val_accuracy: 0.9875 - val_loss: 0.1000\n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 252ms/step - accuracy: 0.9321 - loss: 0.2024 - val_accuracy: 0.9750 - val_loss: 0.0794\n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 249ms/step - accuracy: 0.9455 - loss: 0.1895 - val_accuracy: 0.9875 - val_loss: 0.0918\n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 255ms/step - accuracy: 0.9210 - loss: 0.1952 - val_accuracy: 0.9875 - val_loss: 0.0825\n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 262ms/step - accuracy: 0.9214 - loss: 0.1879 - val_accuracy: 0.9750 - val_loss: 0.1041\n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 254ms/step - accuracy: 0.9604 - loss: 0.1311 - val_accuracy: 0.9875 - val_loss: 0.0697\n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 422ms/step - accuracy: 0.9292 - loss: 0.1596 - val_accuracy: 0.9875 - val_loss: 0.0847\n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 410ms/step - accuracy: 0.8967 - loss: 0.2106 - val_accuracy: 0.9875 - val_loss: 0.0596\n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 409ms/step - accuracy: 0.9198 - loss: 0.1589 - val_accuracy: 0.9875 - val_loss: 0.0540\n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 439ms/step - accuracy: 0.9388 - loss: 0.1806 - val_accuracy: 0.9750 - val_loss: 0.0912\n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 471ms/step - accuracy: 0.9591 - loss: 0.1336 - val_accuracy: 0.9750 - val_loss: 0.0832\n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 460ms/step - accuracy: 0.9217 - loss: 0.1614 - val_accuracy: 0.9625 - val_loss: 0.0978\n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 418ms/step - accuracy: 0.9552 - loss: 0.1646 - val_accuracy: 0.9750 - val_loss: 0.0665\n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 449ms/step - accuracy: 0.9233 - loss: 0.1480 - val_accuracy: 0.9750 - val_loss: 0.0548\n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 419ms/step - accuracy: 0.9648 - loss: 0.1026 - val_accuracy: 0.9750 - val_loss: 0.0756\n"
          ]
        }
      ],
      "source": [
        "history = train_facial_expression_recognition_model(preprocessed_dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# Load the pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.read(\"face_recognition.xml\")\n",
        "model = load_model('facial_expression_recognition.keras')\n",
        "\n",
        "mood = ['Angry', 'Happy', 'Neutral', 'Sad']\n",
        "\n",
        "# Initialize the camera\n",
        "cap = cv2.VideoCapture(0)\n",
        " \n",
        "while True:\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Draw rectangles around the detected faces and recognize them\n",
        "    for (x, y, w, h) in faces:\n",
        "        emotion_label = ''\n",
        "        label_text = ''\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "\n",
        "        id_, confidence = recognizer.predict(roi_gray)\n",
        "        facial_expression_input = resize(extract_face_region(roi_gray)).reshape(140, 140, 1)\n",
        "\n",
        "        if confidence < 70:            \n",
        "            label_text = f\"Ben\"\n",
        "            box_color = (0, 255, 0)\n",
        "\n",
        "            if facial_expression_input.shape == (140, 140, 1):\n",
        "                facial_expression_input = np.expand_dims(facial_expression_input, axis=0)\n",
        "                predictions = model.predict(facial_expression_input)\n",
        "                emotion_label = mood[np.argmax(predictions)]\n",
        "        else:\n",
        "            label_text = f\"Unknown\"\n",
        "            box_color = (0, 0, 255)\n",
        "\n",
        "        cv2.putText(frame, emotion_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        cv2.putText(frame, label_text, (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), box_color, 2)\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow('Mood Detection', frame)\n",
        "\n",
        "    # Break the loop when 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the capture\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image.png](./resources/happy_test.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation of the steps\n",
        "\n",
        "1. I collected a total of 400 (100 each mood), the dataset using `haarcascade_frontalface_default.xml` to detect my face and extract it from the video frame, then saved it in `faces` folder.\n",
        "2. I performed pre-processing on the image to extract face region from all raw datasets, grayscaled it and then resized them to (140, 140) in preparation to use it as training data, then moved it to `preprocessed_faces` folder.\n",
        "3. Using the raw dataset, I trained an LBPH algorithm to perform face recognition, I saved the model in `face_recognition.xml`. The reason I chose LBPH algorithm because it can detect person's face from front and side.\n",
        "4. Using the preprocessed dataset, I trained a CNN model for the facial expression recognition, the dataset was split into 80% and 20% for training and validation.\n",
        "5. With face detection, face recognition, and facial expression recognition, I wrote the code to open a camera and used haar cascade to perform face detection, then extract the face from the grayscaled video frame. the extracted face was inputted in the face recognition. I chose the confidence level of the face recognition to be 70. If the face recognition recognizes me, it then performs the same preprocessing methods that I used to process the dataset so that it can be inputted in the facial expression recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This activity gives me an insight on how OpenCV is used together with Keras to perform both detection and recognition. I are able to visualize the results of the mood detection with the use of OpenCV."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
